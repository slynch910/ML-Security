# -*- coding: utf-8 -*-
"""mls_svm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lMURw3Tqu1Emk2osS_x4CgLfjNw9hzI1
"""

import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix,ConfusionMatrixDisplay
from scipy.io import arff
from numpy.lib import average

iot23 = pd.read_csv("iot_23_dataset.csv", nrows = 75534)
p = LabelEncoder()
iot23.head()

#feature scaling because several of the columns are strings 
iot23['id.orig_h'] = p.fit_transform(iot23['id.orig_h'])
iot23['id.resp_h'] = p.fit_transform(iot23['id.resp_h'])
iot23['proto'] = p.fit_transform(iot23['proto'])
iot23['conn_state'] = p.fit_transform(iot23['conn_state'])
iot23['history'] = p.fit_transform(iot23['history'])
iot23['label'] = p.fit_transform(iot23['label']) 
iot23 = iot23.drop(columns=['uid', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'local_orig', 'local_resp'])
iot23.head()

X = iot23[iot23.columns[0:13]]
Y = iot23["label"]
X

Y

scaler = StandardScaler()
svm = LinearSVC(C = 1, loss = "hinge", max_iter = 1000000, random_state = 42)

scaled_svm = Pipeline([("scaler", scaler), ("linear_svc", svm),])

X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)

scaled_svm.fit(X_train, y_train)

predictions = scaled_svm.predict(X_test)
print("Test Accuracy: {}".format(accuracy_score(y_test, predictions)))
print("Test Precision: {}".format(precision_score(y_test,predictions, average = 'weighted')))
print("Test Recall: {}".format(recall_score(y_test,predictions, average = 'weighted')))

predictions = scaled_svm.predict(X_train)
print("Training Accuracy: {}".format(accuracy_score(y_train, predictions)))
print("Trainign Precision: {}".format(precision_score(y_train,predictions, average = 'weighted')))
print("Training Recall: {}".format(recall_score(y_train,predictions, average = 'weighted')))

cm = confusion_matrix(y_train, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm).plot()

print("Labels:")
print("0 --> Benign")
print("1 --> C&C")
print("2 --> C&C FileDownload")
print("3 --> Okiru")
print("4 --> PartOfAHorizontalPortScan")

#using stratified KFold because data is extremely unbalanced
num_splits = 8
kf = StratifiedKFold(n_splits=num_splits,random_state=42,shuffle=True)
scorings = {"acc" : "accuracy", "prec_macro" : "precision_macro", "rec_macro" : "recall_macro" }

scores = cross_validate(scaled_svm, X, Y, scoring = scorings, cv = kf, return_train_score = True)

print("=======================================================")
i = 0
for i in range(0,num_splits):
  print("Accuracy rating for k={}: {}".format(i+1, scores["test_acc"][i]))
  print("Precision rating for k={}: {}".format(i+1, scores["test_prec_macro"][i]))
  print("Recall rating for k={}: {}".format(i+1, scores["test_rec_macro"][i]))
  print("Training Accuracy rating for k = {}: {}".format(i + 1, scores["train_acc"][i]))
  print("Training Precision rating for k = {}: {}".format(i + 1, scores["train_prec_macro"][i]))
  print("Training Recall rating for k = {}: {}".format(i + 1, scores["train_rec_macro"][i]))

  i += 1
  print("=======================================================")

print("Mean Test Accuracy: {}".format(average(scores["test_acc"])))
print("Mean Test Precision: {}".format(average(scores["test_prec_macro"])))
print("Mean Test Recall: {}".format(average(scores["test_rec_macro"])))
print("\n\n")
print("Mean Training Accuracy: {}".format(average(scores["train_acc"])))
print("Mean Training Precision: {}".format(average(scores["train_prec_macro"])))
print("Mean Training Recall: {}".format(average(scores["train_rec_macro"])))

uci = pd.read_csv("uci_dataset.csv")
uci.head()

X2 = uci[uci.columns[0:115]]
Y2 = uci['Label']

X2

Y2

scaler2 = StandardScaler()
svm2 = LinearSVC(C = 1, loss = "hinge", max_iter = 10000000, random_state = 42)
scaled_svm2 = Pipeline([("scaler", scaler2), ("linear_svc", svm2),])
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,Y2, test_size = 0.3, random_state = 42)

scaled_svm2.fit(X_train2, y_train2)

predictions2 = scaled_svm2.predict(X_test2)
print("Test Accuracy: {}".format(accuracy_score(y_test2, predictions2)))
print("Test Precision: {}".format(precision_score(y_test2,predictions2, average = 'weighted')))
print("Test Recall: {}".format(recall_score(y_test2,predictions2, average = 'weighted')))

predictions2 = scaled_svm2.predict(X_train2)
print("Training Accuracy: {}".format(accuracy_score(y_train2, predictions2)))
print("Training Precision: {}".format(precision_score(y_train2,predictions2, average = 'weighted')))
print("Training Recall: {}".format(recall_score(y_train2,predictions2, average = 'weighted')))

cm = confusion_matrix(y_train2, predictions2)
disp = ConfusionMatrixDisplay(confusion_matrix=cm).plot()

num_splits2 = 10
kf2 = StratifiedKFold(n_splits=num_splits2,random_state=42,shuffle=True)
scorings2 = {"acc" : "accuracy", "prec_macro" : "precision_macro", "rec_macro" : "recall_macro" }
scores2 = cross_validate(scaled_svm2, X2, Y2, scoring = scorings2, cv = kf2, return_train_score = True)

print("=======================================================")
i = 0
for i in range(0,num_splits):
  print("Accuracy rating for k={}: {}".format(i+1, scores2["test_acc"][i]))
  print("Precision rating for k={}: {}".format(i+1, scores2["test_prec_macro"][i]))
  print("Recall rating for k={}: {}".format(i+1, scores2["test_rec_macro"][i]))
  print("Training Accuracy rating for k = {}: {}".format(i + 1, scores2["train_acc"][i]))
  print("Training Precision rating for k = {}: {}".format(i + 1, scores2["train_prec_macro"][i]))
  print("Training Recall rating for k = {}: {}".format(i + 1, scores2["train_rec_macro"][i]))

  i += 1
  print("=======================================================")

print("Mean Test Accuracy: {}".format(average(scores2["test_acc"])))
print("Mean Test Precision: {}".format(average(scores2["test_prec_macro"])))
print("Mean Test Recall: {}".format(average(scores2["test_rec_macro"])))
print("\n\n")
print("Mean Training Accuracy: {}".format(average(scores2["train_acc"])))
print("Mean Training Precision: {}".format(average(scores2["train_prec_macro"])))
print("Mean Training Recall: {}".format(average(scores2["train_rec_macro"])))

nsl_kdd =  pd.DataFrame(arff.loadarff("KDDTrain+.arff")[0])
for cols in ["protocol_type", "service", "flag", "class"]: 
            nsl_kdd[cols] = nsl_kdd[cols].str.decode("utf-8")

nsl_kdd= nsl_kdd.replace({"protocol_type": {"tcp": 0, "udp": 1, "icmp": 2,}})
nsl_kdd = nsl_kdd.replace({"class": {"normal": 0, "anomaly": 1}})

categories_that_need_mappings = ["service", "flag"]
for category in categories_that_need_mappings: 
  mappings = {}
  for i, value in enumerate(list(set(nsl_kdd[category]))):
    mappings.update({value: i}) 

  nsl_kdd = nsl_kdd.replace({category: mappings})

nsl_kdd.head()

X3 = nsl_kdd[nsl_kdd.columns[0:41]]
Y3 = nsl_kdd['class']
X3

scaler3 = StandardScaler()
svm3 = LinearSVC(C = 1, loss = "hinge", max_iter = 10000000, random_state = 42)
scaled_svm3 = Pipeline([("scaler", scaler3), ("linear_svc", svm3),])
X_train3, X_test3, y_train3, y_test3 = train_test_split(X3,Y3, test_size = 0.4, random_state = 42)

scaled_svm3.fit(X_train3, y_train3)

predictions3 = scaled_svm3.predict(X_test3)
print("Test Accuracy: {}".format(accuracy_score(y_test3, predictions3)))
print("Test Precision: {}".format(precision_score(y_test3,predictions3, average = 'weighted')))
print("Test Recall: {}".format(recall_score(y_test3,predictions3, average = 'weighted')))

predictions3 = scaled_svm3.predict(X_train3)
print("Training Accuracy: {}".format(accuracy_score(y_train3, predictions3)))
print("Training Precision: {}".format(precision_score(y_train3,predictions3, average = 'weighted')))
print("Training Recall: {}".format(recall_score(y_train3,predictions3, average = 'weighted')))

cm = confusion_matrix(y_train3, predictions3)
disp = ConfusionMatrixDisplay(confusion_matrix=cm).plot()

num_splits3 = 10
kf3 = StratifiedKFold(n_splits=num_splits3,random_state=42,shuffle=True)
scorings3 = {"acc" : "accuracy", "prec_macro" : "precision_macro", "rec_macro" : "recall_macro" }
scores3 = cross_validate(scaled_svm3, X3, Y3, scoring = scorings3, cv = kf3, return_train_score = True)

print("=======================================================")
i = 0
for i in range(0,num_splits3):
  print("Accuracy rating for k={}: {}".format(i+1, scores3["test_acc"][i]))
  print("Precision rating for k={}: {}".format(i+1, scores3["test_prec_macro"][i]))
  print("Recall rating for k={}: {}".format(i+1, scores3["test_rec_macro"][i]))
  print("Training Accuracy rating for k = {}: {}".format(i + 1, scores3["train_acc"][i]))
  print("Training Precision rating for k = {}: {}".format(i + 1, scores3["train_prec_macro"][i]))
  print("Training Recall rating for k = {}: {}".format(i + 1, scores3["train_rec_macro"][i]))

  i += 1
  print("=======================================================")

print("Mean Test Accuracy: {}".format(average(scores3["test_acc"])))
print("Mean Test Precision: {}".format(average(scores3["test_prec_macro"])))
print("Mean Test Recall: {}".format(average(scores3["test_rec_macro"])))
print("\n\n")
print("Mean Training Accuracy: {}".format(average(scores3["train_acc"])))
print("Mean Training Precision: {}".format(average(scores3["train_prec_macro"])))
print("Mean Training Recall: {}".format(average(scores3["train_rec_macro"])))

kitsune = pd.read_csv("mirai_combined.csv", header=0)
kitsune.head()

X4 = kitsune[kitsune.columns[0:116]]
Y4 = kitsune['Verdict']
X4

Y4

scaler4 = StandardScaler()
svm4 = LinearSVC(C = .0000875, loss = "hinge", max_iter = 1000000, random_state = 42)
scaled_svm4 = Pipeline([("scaler", scaler4), ("linear_svc", svm4),])
X_train4, X_test4, y_train4, y_test4 = train_test_split(X4,Y4, test_size = 0.3, random_state = 42)

scaled_svm4.fit(X_train4, y_train4)

predictions4 = scaled_svm4.predict(X_test4)
print("Test Accuracy: {}".format(accuracy_score(y_test4, predictions4)))
print("Test Precision: {}".format(precision_score(y_test4,predictions4, average = 'weighted')))
print("Test Recall: {}".format(recall_score(y_test4,predictions4, average = 'weighted')))

predictions4 = scaled_svm4.predict(X_train4)
print("Training Accuracy: {}".format(accuracy_score(y_train4, predictions4)))
print("Training Precision: {}".format(precision_score(y_train4,predictions4, average = 'weighted')))
print("Training Recall: {}".format(recall_score(y_train4,predictions4, average = 'weighted')))

cm = confusion_matrix(y_train4, predictions4)
disp = ConfusionMatrixDisplay(confusion_matrix=cm).plot()

num_splits4 = 10
kf4 = StratifiedKFold(n_splits=num_splits4,random_state=42,shuffle=True)
scorings4 = {"acc" : "accuracy", "prec_macro" : "precision_macro", "rec_macro" : "recall_macro" }
scores4 = cross_validate(scaled_svm4, X4, Y4, scoring = scorings4, cv = kf4, return_train_score = True)

print("=======================================================")
i = 0
for i in range(0,num_splits4):
  print("Accuracy rating for k={}: {}".format(i+1, scores4["test_acc"][i]))
  print("Precision rating for k={}: {}".format(i+1, scores4["test_prec_macro"][i]))
  print("Recall rating for k={}: {}".format(i+1, scores4["test_rec_macro"][i]))
  print("Training Accuracy rating for k = {}: {}".format(i + 1, scores4["train_acc"][i]))
  print("Training Precision rating for k = {}: {}".format(i + 1, scores4["train_prec_macro"][i]))
  print("Training Recall rating for k = {}: {}".format(i + 1, scores4["train_rec_macro"][i]))

  i += 1
  print("=======================================================")

print("Mean Test Accuracy: {}".format(average(scores4["test_acc"])))
print("Mean Test Precision: {}".format(average(scores4["test_prec_macro"])))
print("Mean Test Recall: {}".format(average(scores4["test_rec_macro"])))
print("\n\n")
print("Mean Training Accuracy: {}".format(average(scores4["train_acc"])))
print("Mean Training Precision: {}".format(average(scores4["train_prec_macro"])))
print("Mean Training Recall: {}".format(average(scores4["train_rec_macro"])))